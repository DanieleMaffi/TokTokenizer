{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac93490-7d59-4742-a77c-6fa7fe7bd1fa",
   "metadata": {},
   "source": [
    "# GPT Tokenizer\n",
    "After following Andrej Karpathy lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bdbe6b-5518-4693-bc98-60bf615f10bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample text from https://x.com/aelluswamy/status/1981644831790379245\n",
    "\n",
    "text = \"\"\"I had the pleasure of presenting some of the recent work on behalf of the @Tesla_AI team at the International Conference of Computer Vision this week. In this abridged version of the talk, we will go over some of the essentials of Tesla's approach to solving autonomy.\n",
    "As many of you know, Tesla utilizes an end-to-end neural network to achieve self-driving. This end-to-end neural network consumes pixels from the various cameras, kinematic signals (such as the vehicle speed), audio, maps and navigation to produce the control commands that drive the car.\n",
    "Why end-to-end?\n",
    "\n",
    "RANDOM UNICODE EMOJIS ðŸ™„ðŸ™„ðŸ™„ðŸ™„\n",
    "\n",
    "Even though Tesla strongly believes in end-to-end neural networks, it is by no means the consensus approach to self-driving. Most other entities developing self-driving have a sensor-heavy, modular approach to driving. While such systems may be easier to develop and debug in the beginning, there are several complexities with such a system. The end-to-end approach offers several benefits over that baseline. To name a few:\n",
    "Codifying human values is incredibly difficult. It is easier to learn them from data.\n",
    "Interface between perception, prediction and planning is ill-defined. In end-to-end, the gradients flow all the way from controls to sensor inputs, thus optimizing the entire network holistically.\n",
    "Easily scalable to handle the fat and long tail f real-world robotics.\n",
    "Homogenous compute with deterministic latency.\n",
    "Overall, on the correct side of scaling w.r.t. the bitter lesson.\n",
    "Here are a few examples that illustrate this.\n",
    "Example 1:\n",
    "In the below example, the AI needs to decide between going over the large puddle or entering the oncoming lane. Typically, entering oncoming lanes can be very bad and potentially dangerous. However, in this situation there's enough visibility to know that there wouldn't be an oncoming car in the foreseeable future. Secondly, that the puddle is rather large and is better avoided. Such trade-offs cannot be easily written down in traditional programming logic, whereas it's rather straightforward for a human looking at the scene. \n",
    "\n",
    "0:00 / 0:18\n",
    "Example 1 for why end-to-end. Video from https://x.com/AIDRIVR/status/1760841783708418094\n",
    "The classic trolley problem is conventionally considered as a rare problem that self-driving cars would seldom face. However, the opposite is what's true. Self-driving cars are constantly subject to mini-trolley problems such as the one above. By training on human data, the robots learn values that are aligned with what humans value.\n",
    "Example 2:\n",
    "It's hard to have a clear interface between \"perception\" and \"planning\". In the below two clips, in one the chickens want to cross the road and in the other the geese just want to hang out. It is quite difficult to create an ontology for this between these modular blocks. This kind of soft \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af02af0-fef3-45cd-ba55-57ee98983114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.merges = None\n",
    "    \n",
    "    def _get_stats(self, ids, stats=None):\n",
    "        \"\"\"Get counts for each pair.\"\"\"\n",
    "        stats = {} if stats is None else stats\n",
    "        for b1, b2 in zip(ids, ids[1:]): # This is a way to iterate pairs\n",
    "            stats[(b1, b2)] = stats.get((b1, b2), 0) + 1\n",
    "        return stats\n",
    "    \n",
    "    def _merge(self, ids, pair, idx):\n",
    "        \"\"\"Merge a pair into a single index idx.\"\"\"\n",
    "        new_ids = []\n",
    "        \n",
    "        merged = False\n",
    "        for b1, b2 in zip(ids, ids[1:]):\n",
    "            \n",
    "            # If there was a merge we need to skip also the next byte\n",
    "            if merged:\n",
    "                merged = False\n",
    "                continue\n",
    "            \n",
    "            if pair == (b1, b2):\n",
    "                new_ids.append(idx)\n",
    "                merged = True\n",
    "            else:\n",
    "                new_ids.append(b1)\n",
    "                \n",
    "        # Append last element if did not merge, otherwise it would be lost\n",
    "        if not merged: \n",
    "            new_ids.append(ids[-1])\n",
    "            \n",
    "        return new_ids\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        merges = {}\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        num_merges = vocab_size - 256 # number of single byte values\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Text length: {len(ids)} bytes\")\n",
    "            print(f\"Will do {num_merges} merges\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            idx1, idx2 = max(stats, key=stats.get)\n",
    "            minted = len(vocab)\n",
    "            ids = self._merge(ids, (idx1, idx2), minted)\n",
    "            merges[(idx1, idx2)] = minted\n",
    "            vocab[minted] = vocab[idx1] + vocab[idx2]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{i+1}/{num_merges} - {(i+1)/num_merges:.2%}\")\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "            \n",
    "        return vocab, merges\n",
    "        \n",
    "    def encode(self, text):\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        while True:\n",
    "            if len(ids) < 2:\n",
    "                break\n",
    "            \n",
    "            bigrams = zip(ids, ids[1:])\n",
    "            # Find the pair with the lowest index\n",
    "            pair = min(bigrams, key=lambda pair: self.merges.get(pair, float('inf')))\n",
    "            \n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "                \n",
    "            ids = self._merge(ids, pair, self.merges[pair])\n",
    "        \n",
    "        return ids\n",
    "            \n",
    "    def decode(self, ids):\n",
    "        encoded = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        return encoded.decode(\"utf-8\", errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b8f7c3a-cfc9-4d02-9cc0-af2d3eb5b56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 - 100.00%\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "v, m = tokenizer.train(text, 500, verbose=True)\n",
    "# tokenizer._merge([1, 2, 3, 4, 4, 3, 4, 3, 45, 6, 3, 4], (3, 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0210fd4e-b8c1-40cd-9d08-144ffa7e90f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 79, 86, 69, 32, 353, 33, 33, 33, 33, 32, 226, 143, 179]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"LOVE end-to-end !!!! â³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cea5bf1a-2052-4f80-85b0-e0e0a6130100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOVE end-to-end !!!! â³'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"LOVE end-to-end !!!! â³\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60381ad9-3d9f-41e4-94a4-fd1dd23908cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6799b185-6c77-4a98-b4c4-9b4c97c15200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba05bc6-32a0-4edd-a8ac-861142715c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "\n",
    "class RegexTokenizer(BasicTokenizer):\n",
    "    GPT4_SPLIT_PATTERN = (\n",
    "        r\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}|\"\n",
    "        \" ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\n",
    "    )\n",
    "    \n",
    "    def __init__(self, regex=None):\n",
    "        self.regex = regex or self.GPT4_SPLIT_PATTERN\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        chunks = regex.findall(self.regex, text)\n",
    "        chunks_ids = list(chunk.encode(\"utf-8\") for chunk in chunks)\n",
    "        \n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        merges = {}\n",
    "        \n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            stats = {}\n",
    "            for cids in chunks_ids:\n",
    "                # Update stats in-place\n",
    "                self._get_stats(cids, stats)\n",
    "                \n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # Get pair with highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            minted = len(vocab)\n",
    "            # Update the chunks list by merging the pair of each chunk\n",
    "            chunks_ids = [self._merge(cids, pair, minted) for cids in chunks_ids]\n",
    "            merges[pair] = minted\n",
    "            id1, id2 = pair\n",
    "            vocab[minted] = vocab[id1] + vocab[id2]\n",
    "            \n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"{i+1}/{num_merges} - {(i+1)/num_merges:.2%}\")\n",
    "                print(f\"{pair} -> {minted} ({vocab[minted]}): had {stats[pair]} occurrences\")\n",
    "                \n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        \n",
    "        return vocab, merges\n",
    "    \n",
    "    def encode(self, text):\n",
    "        chunks = regex.findall(self.regex, text)\n",
    "        ids = []\n",
    "        for chunk in chunks:\n",
    "            ids.extend(super().encode(chunk))\n",
    "        return ids\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "430c5333-e05f-4d4c-a6c2-73bc7c3e9179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 - 100.00%\n",
      "(498, 372) -> 499 (b' difficul'): had 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "v, m = tokenizer.train(text, 500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8ceb6ae-acdf-4164-a551-69dd53fe90f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 79, 86, 69, 318, 319, 320, 32, 33, 33, 33, 33, 32, 226, 143, 179, 499]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"LOVE end-to-end !!!! â³ difficul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34fda950-a597-441b-960f-ebd49a3a4f28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOVE end-to-end !!!! â³'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"LOVE end-to-end !!!! â³\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bf6cbcf-36a6-4db6-a282-8a391e2e35eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0936dae1-af9f-4459-bc05-60f20a218118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c8ce79c-0f95-4238-9ede-3cb15ca0b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4Tokenizer(RegexTokenizer):\n",
    "    def __init__(self, special_tokens=None):\n",
    "        super().__init__()\n",
    "        self.special_tokens = special_tokens or {}\n",
    "        self.inverse_special_tokens = {v: k for k, v in self.special_tokens.items()}\n",
    "        # Prepend speacial tokens to regex\n",
    "        self.special_tokens_regex = fr\"(?:{'|'.join(special_tokens)})|\"\n",
    "        \n",
    "    def encode(self, text):\n",
    "        ids = []\n",
    "        for split in regex.split(fr\"({'|'.join(self.special_tokens)})\", text):\n",
    "            if split in self.special_tokens:\n",
    "                ids.append(self.special_tokens[split])\n",
    "            else:\n",
    "                ids.extend(super().encode(split))\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        parts = []\n",
    "        for idx in ids:\n",
    "            if idx in self.inverse_special_tokens:\n",
    "                parts.append(self.inverse_special_tokens[idx])\n",
    "            else:\n",
    "                parts.append(self.vocab[idx].decode(\"utf-8\"))\n",
    "        return \"\".join(parts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8561d4c-320f-4d0b-9b34-41ce79ed2097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 - 100.00%\n",
      "(498, 372) -> 499 (b' difficul'): had 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT4Tokenizer({\"<pad>\": 501})\n",
    "v, m = tokenizer.train(text, 500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4244f258-fbfb-444a-96f2-5f38544a66d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 69, 76, 76, 79, 33, 32, 501]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"HELLO! <pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c8370ab-4e19-4178-9ecd-dbab2b0fd494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO! <pad>'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"HELLO! <pad>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65354c-f6b8-4536-8216-c469c4754cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
